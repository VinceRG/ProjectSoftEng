{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86952d98",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 101\u001b[39m\n\u001b[32m     90\u001b[39m         results.append({\n\u001b[32m     91\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mconsult_type\u001b[39m\u001b[33m\"\u001b[39m: consult_type,\n\u001b[32m     92\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mconsult_type_name\u001b[39m\u001b[33m\"\u001b[39m: type_name,\n\u001b[32m   (...)\u001b[39m\u001b[32m     97\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mspearman_corr\u001b[39m\u001b[33m\"\u001b[39m: spearman_corr\n\u001b[32m     98\u001b[39m         })\n\u001b[32m    100\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m results = \u001b[43mevaluate_top_cases_by_type\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m12\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mevaluate_top_cases_by_type\u001b[39m\u001b[34m(test_year, test_month)\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mglobal\u001b[39;00m df, model, X_train_columns\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Filter training/test data\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m train_df = \u001b[43mdf\u001b[49m[(df[\u001b[33m'\u001b[39m\u001b[33mYear\u001b[39m\u001b[33m'\u001b[39m] < test_year) | ((df[\u001b[33m'\u001b[39m\u001b[33mYear\u001b[39m\u001b[33m'\u001b[39m] == test_year) & (df[\u001b[33m'\u001b[39m\u001b[33mMonth\u001b[39m\u001b[33m'\u001b[39m] < test_month))]\n\u001b[32m     14\u001b[39m test_df = df[(df[\u001b[33m'\u001b[39m\u001b[33mYear\u001b[39m\u001b[33m'\u001b[39m] == test_year) & (df[\u001b[33m'\u001b[39m\u001b[33mMonth\u001b[39m\u001b[33m'\u001b[39m] == test_month)]\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m test_df.empty:\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "CONSULTATION_MAP = {1: \"Consultation\", 2: \"Diagnosis\", 3: \"Mortality\"}\n",
    "\n",
    "def evaluate_top_cases_by_type(test_year, test_month):\n",
    "    \"\"\"Evaluate prediction accuracy for each consultation type separately.\"\"\"\n",
    "    global df, model, X_train_columns\n",
    "\n",
    "    # Filter training/test data\n",
    "    train_df = df[(df['Year'] < test_year) | ((df['Year'] == test_year) & (df['Month'] < test_month))]\n",
    "    test_df = df[(df['Year'] == test_year) & (df['Month'] == test_month)]\n",
    "    if test_df.empty:\n",
    "        print(f\"No data available for {test_year}-{test_month:02d}\")\n",
    "        return None\n",
    "\n",
    "    print(f\"\\n=== Evaluating predictions for {test_year}-{test_month:02d} ===\")\n",
    "    results = []\n",
    "\n",
    "    for consult_type in sorted(test_df['Consultation_Type'].unique()):\n",
    "        type_name = CONSULTATION_MAP.get(consult_type, f\"Type {consult_type}\")\n",
    "        print(f\"\\nðŸ©º Consultation Type {consult_type}: {type_name}\")\n",
    "\n",
    "        train_type_df = train_df[train_df['Consultation_Type'] == consult_type]\n",
    "        test_type_df = test_df[test_df['Consultation_Type'] == consult_type]\n",
    "        if len(test_type_df) < 10:\n",
    "            print(f\"  âš  Skipping â€” not enough data for {type_name}\")\n",
    "            continue\n",
    "\n",
    "        X_train = train_type_df.drop(columns=['Total'])\n",
    "        y_train = train_type_df['Total']\n",
    "        X_test  = test_type_df.drop(columns=['Total'])\n",
    "        y_test  = test_type_df['Total']\n",
    "\n",
    "        # Train a temporary model for that type\n",
    "        rf = RandomForestRegressor(\n",
    "            n_estimators=100,\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            min_samples_leaf=5\n",
    "        )\n",
    "        rf.fit(X_train, y_train)\n",
    "\n",
    "        # Predict on test set\n",
    "        y_pred = rf.predict(X_test)\n",
    "        y_pred[y_pred < 0] = 0\n",
    "        test_type_df = test_type_df.copy()\n",
    "        test_type_df['Predicted_Total'] = y_pred\n",
    "\n",
    "        # Regression metrics\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "        # Compute actual & predicted top cases\n",
    "        actual_top = (\n",
    "            test_type_df.groupby('Case')['Total'].sum()\n",
    "            .sort_values(ascending=False)\n",
    "            .head(10)\n",
    "            .index.tolist()\n",
    "        )\n",
    "\n",
    "        predicted_top = (\n",
    "            test_type_df.groupby('Case')['Predicted_Total'].sum()\n",
    "            .sort_values(ascending=False)\n",
    "            .head(10)\n",
    "            .index.tolist()\n",
    "        )\n",
    "\n",
    "        overlap = len(set(actual_top) & set(predicted_top))\n",
    "        top10_accuracy = overlap / 10 * 100\n",
    "\n",
    "        # Rank correlation\n",
    "        merged = pd.merge(\n",
    "            test_type_df.groupby('Case')['Total'].sum(),\n",
    "            test_type_df.groupby('Case')['Predicted_Total'].sum(),\n",
    "            left_index=True, right_index=True\n",
    "        )\n",
    "        spearman_corr, _ = spearmanr(merged['Total'], merged['Predicted_Total'])\n",
    "\n",
    "        # Print results\n",
    "        print(f\"  MAE: {mae:.3f}\")\n",
    "        print(f\"  RÂ²: {r2:.3f}\")\n",
    "        print(f\"  Top-10 Overlap: {overlap}/10 ({top10_accuracy:.1f}%)\")\n",
    "        print(f\"  Spearman Rank Corr: {spearman_corr:.3f}\")\n",
    "        print(f\"  Actual Top: {actual_top}\")\n",
    "        print(f\"  Predicted Top: {predicted_top}\")\n",
    "\n",
    "        results.append({\n",
    "            \"consult_type\": consult_type,\n",
    "            \"consult_type_name\": type_name,\n",
    "            \"mae\": mae,\n",
    "            \"r2\": r2,\n",
    "            \"top10_overlap\": overlap,\n",
    "            \"top10_accuracy_percent\": top10_accuracy,\n",
    "            \"spearman_corr\": spearman_corr\n",
    "        })\n",
    "\n",
    "    return results\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"master_dataset_cleaned.csv\")\n",
    "results = evaluate_top_cases_by_type(2024, 12)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
